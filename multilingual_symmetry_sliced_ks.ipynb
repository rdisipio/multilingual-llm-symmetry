{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9f04e8",
   "metadata": {},
   "source": [
    "# Multilingual Symmetry Evaluation with Sliced KS\n",
    "\n",
    "This notebook implements the core pieces of a **multilingual symmetry** evaluation pipeline:\n",
    "\n",
    "1. Generate multiple answers per prompt, per `(model, language)` condition.\n",
    "2. Encode answers with a **shared multilingual embedding model**.\n",
    "3. Compute a **Sliced Kolmogorov–Smirnov (sKS)** distance between the two embedding distributions.\n",
    "4. Aggregate sKS across random projection directions to get a **mean symmetry score with uncertainty**.\n",
    "\n",
    "The code is structured so you can plug in your own LLM backends and your own prompt sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1971e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: install dependencies (uncomment if needed)\n",
    "# You can skip this if you already have these packages in your environment.\n",
    "\n",
    "# !pip install sentence-transformers scipy numpy pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbdd530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "np.random.seed(12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf2257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multilingual embeddings\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b7396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Example prompts for quick testing (replace with dataset-driven prompts later)\n",
    "PROMPTS = [\n",
    "    # Factual\n",
    "    {\n",
    "        \"id\": \"fact_1\",\n",
    "        \"en\": \"Who discovered penicillin?\",\n",
    "        \"fr\": \"Qui a découvert la pénicilline ?\",\n",
    "        \"type\": \"factual\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"fact_2\",\n",
    "        \"en\": \"What is the capital of Japan?\",\n",
    "        \"fr\": \"Quelle est la capitale du Japon ?\",\n",
    "        \"type\": \"factual\",\n",
    "    },\n",
    "    # Open-ended / intent-like\n",
    "    {\n",
    "        \"id\": \"open_1\",\n",
    "        \"en\": \"How would you resolve a conflict between two colleagues at work?\",\n",
    "        \"fr\": \"Comment résoudriez-vous un conflit entre deux collègues au travail ?\",\n",
    "        \"type\": \"open\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Define conditions you want to compare.\n",
    "# For example, same model but different languages:\n",
    "CONDITIONS = [\n",
    "    {\"name\": \"modelA_en\", \"model\": \"MODEL_A_NAME\", \"lang\": \"en\"},\n",
    "    {\"name\": \"modelA_fr\", \"model\": \"MODEL_A_NAME\", \"lang\": \"fr\"},\n",
    "    # You can add more, e.g. another model:\n",
    "    # {\"name\": \"modelB_en\", \"model\": \"MODEL_B_NAME\", \"lang\": \"en\"},\n",
    "    # {\"name\": \"modelB_fr\", \"model\": \"MODEL_B_NAME\", \"lang\": \"fr\"},\n",
    "]\n",
    "\n",
    "# Pairs of conditions to compare with the symmetry metric\n",
    "CONDITION_PAIRS = [\n",
    "    (\"modelA_en\", \"modelA_fr\"),\n",
    "    # e.g. (\"modelA_en\", \"modelB_en\"),\n",
    "    # e.g. (\"modelA_fr\", \"modelB_fr\"),\n",
    "]\n",
    "\n",
    "# Number of stochastic samples per (prompt, condition)\n",
    "N_SAMPLES_PER_CONDITION = 16\n",
    "\n",
    "# Number of random projection directions for sliced KS\n",
    "N_DIRECTIONS = 64\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 123\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5203cc96",
   "metadata": {},
   "source": [
    "## Answer generation\n",
    "\n",
    "In this section, we define a stub for generating multiple answers from each model.\n",
    "You should plug in your own LLM backend (OpenAI, Cohere, Anthropic, local models, etc.).\n",
    "\n",
    "By default, we use a dummy implementation that echoes the prompt and appends a random suffix,\n",
    "so that the rest of the pipeline can be tested without external APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c7422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_answers_for_condition(\n",
    "    prompt_en: str,\n",
    "    prompt_fr: str,\n",
    "    condition: Dict,\n",
    "    n_samples: int = 8,\n",
    ") -> List[str]:\n",
    "    \"\"\"Generate answers for a single condition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt_en : str\n",
    "        English version of the prompt (semantic base).\n",
    "    prompt_fr : str\n",
    "        French version of the prompt (semantic base).\n",
    "    condition : dict\n",
    "        Contains at least 'name', 'model', 'lang' keys.\n",
    "    n_samples : int\n",
    "        How many answers to sample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        List of model outputs as text.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Replace the body of this function with actual calls to your LLM(s).\n",
    "    For example:\n",
    "        - OpenAI ChatCompletions\n",
    "        - Cohere chat.generate\n",
    "        - Anthropic client.messages.create\n",
    "    \"\"\"\n",
    "    lang = condition[\"lang\"]\n",
    "    model_name = condition[\"model\"]\n",
    "\n",
    "    # Choose the prompt according to language\n",
    "    if lang == \"en\":\n",
    "        prompt = prompt_en\n",
    "    elif lang == \"fr\":\n",
    "        prompt = prompt_fr\n",
    "    else:\n",
    "        # Extend as needed for other languages\n",
    "        prompt = prompt_en\n",
    "\n",
    "    # TODO: Replace this dummy implementation with real API calls.\n",
    "    # ------------------------------------------------------------------\n",
    "    # Example pseudo-code for Cohere (commented out):\n",
    "    #\n",
    "    # from cohere import Client\n",
    "    # co = Client(api_key=\"YOUR_API_KEY\")\n",
    "    # outputs = []\n",
    "    # for _ in range(n_samples):\n",
    "    #     resp = co.chat(\n",
    "    #         model=model_name,\n",
    "    #         message=prompt,\n",
    "    #         temperature=0.7,\n",
    "    #     )\n",
    "    #     outputs.append(resp.text)\n",
    "    # return outputs\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # Dummy implementation: echo + random integer suffix\n",
    "    outputs = []\n",
    "    for i in range(n_samples):\n",
    "        outputs.append(f\"[{condition['name']}] {prompt} (sample {i})\")\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd2408",
   "metadata": {},
   "source": [
    "## Multilingual embedding model\n",
    "\n",
    "We now load a single **multilingual sentence encoder** that will embed **all** answers\n",
    "into a shared semantic vector space. This decouples the evaluation geometry from any particular LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acda4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/LaBSE\"  # or any multilingual model you like\n",
    "\n",
    "def load_embedding_model(model_name: str = EMBEDDING_MODEL_NAME):\n",
    "    if SentenceTransformer is None:\n",
    "        raise ImportError(\n",
    "            \"sentence-transformers is not installed. \"\n",
    "            \"Install it with `pip install sentence-transformers`.\"\n",
    "        )\n",
    "    print(f\"Loading embedding model: {model_name}\")\n",
    "    return SentenceTransformer(model_name)\n",
    "\n",
    "embedding_model = load_embedding_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a19eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_texts(texts: List[str]):\n",
    "    \"\"\"Encode a list of strings into embeddings (np.ndarray of shape [N, D]).\"\"\"\n",
    "    if not texts:\n",
    "        return np.zeros((0, embedding_model.get_sentence_embedding_dimension()))\n",
    "    emb = embedding_model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d69031",
   "metadata": {},
   "source": [
    "## Sliced Kolmogorov–Smirnov (sKS) distance\n",
    "\n",
    "We now implement the **sliced KS** metric:\n",
    "\n",
    "1. Draw `L` random unit vectors in embedding space.\n",
    "2. Project the two sets of embeddings onto each vector.\n",
    "3. Compute the 1D KS distance per direction.\n",
    "4. Aggregate over directions to get a mean, standard deviation, and confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class SlicedKSMetrics:\n",
    "    mean: float\n",
    "    std: float\n",
    "    sem: float\n",
    "    ci_low: float\n",
    "    ci_high: float\n",
    "    per_direction: np.ndarray\n",
    "\n",
    "\n",
    "def _ks_1d(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"1D Kolmogorov–Smirnov distance between two samples.\n",
    "\n",
    "    If scipy is available, we use ks_2samp; otherwise we fall back to a simple implementation.\n",
    "    \"\"\"\n",
    "    if a.size == 0 or b.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    if ks_2samp is not None:\n",
    "        return ks_2samp(a, b, alternative=\"two-sided\", mode=\"auto\").statistic\n",
    "\n",
    "    # Fallback: manual KS\n",
    "    a_sorted = np.sort(a)\n",
    "    b_sorted = np.sort(b)\n",
    "    data_all = np.concatenate([a_sorted, b_sorted])\n",
    "\n",
    "    # Empirical CDFs\n",
    "    cdf_a = np.searchsorted(a_sorted, data_all, side=\"right\") / a_sorted.size\n",
    "    cdf_b = np.searchsorted(b_sorted, data_all, side=\"right\") / b_sorted.size\n",
    "\n",
    "    return np.max(np.abs(cdf_a - cdf_b))\n",
    "\n",
    "\n",
    "def sliced_ks_distance(\n",
    "    emb_A: np.ndarray,\n",
    "    emb_B: np.ndarray,\n",
    "    n_directions: int = 64,\n",
    "    random_state: Optional[int] = None,\n",
    ") -> SlicedKSMetrics:\n",
    "    \"\"\"Compute Sliced Kolmogorov–Smirnov distance between two embedding sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    emb_A : np.ndarray\n",
    "        Embeddings of set A, shape (N_A, D).\n",
    "    emb_B : np.ndarray\n",
    "        Embeddings of set B, shape (N_B, D).\n",
    "    n_directions : int\n",
    "        Number of random projection directions.\n",
    "    random_state : int, optional\n",
    "        Seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    SlicedKSMetrics\n",
    "        Container with mean, std, sem, CI, and per-direction KS values.\n",
    "    \"\"\"\n",
    "    if emb_A.size == 0 or emb_B.size == 0:\n",
    "        return SlicedKSMetrics(\n",
    "            mean=0.0, std=0.0, sem=0.0, ci_low=0.0, ci_high=0.0,\n",
    "            per_direction=np.zeros(n_directions)\n",
    "        )\n",
    "\n",
    "    assert emb_A.shape[1] == emb_B.shape[1], \"Embedding dimensions must match.\"\n",
    "    d = emb_A.shape[1]\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    ks_values = []\n",
    "    for _ in range(n_directions):\n",
    "        # Sample a random direction on the unit sphere\n",
    "        v = rng.normal(size=d)\n",
    "        v /= np.linalg.norm(v) + 1e-12\n",
    "\n",
    "        proj_A = emb_A @ v\n",
    "        proj_B = emb_B @ v\n",
    "\n",
    "        ks_val = _ks_1d(proj_A, proj_B)\n",
    "        ks_values.append(ks_val)\n",
    "\n",
    "    ks_values = np.array(ks_values)\n",
    "    mean = float(ks_values.mean())\n",
    "    std = float(ks_values.std(ddof=1)) if ks_values.size > 1 else 0.0\n",
    "    sem = float(std / np.sqrt(ks_values.size)) if ks_values.size > 0 else 0.0\n",
    "    # 95% CI (approx)\n",
    "    ci_low = max(0.0, mean - 1.96 * sem)\n",
    "    ci_high = min(1.0, mean + 1.96 * sem)\n",
    "\n",
    "    return SlicedKSMetrics(\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "        sem=sem,\n",
    "        ci_low=ci_low,\n",
    "        ci_high=ci_high,\n",
    "        per_direction=ks_values,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7005a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def symmetry_from_sks(metrics: SlicedKSMetrics) -> Dict[str, float]:\n",
    "    \"\"\"Convert sliced KS metrics into a symmetry-oriented view.\n",
    "\n",
    "    Symmetry is defined as 1 - mean KS, with the same uncertainty structure.\n",
    "    \"\"\"\n",
    "    sym_mean = 1.0 - metrics.mean\n",
    "    sym_ci_low = 1.0 - metrics.ci_high\n",
    "    sym_ci_high = 1.0 - metrics.ci_low\n",
    "    return {\n",
    "        \"sym_mean\": sym_mean,\n",
    "        \"sym_std\": metrics.std,\n",
    "        \"sym_sem\": metrics.sem,\n",
    "        \"sym_ci_low\": sym_ci_low,\n",
    "        \"sym_ci_high\": sym_ci_high,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc786e27",
   "metadata": {},
   "source": [
    "## Evaluation loop\n",
    "\n",
    "This cell ties everything together:\n",
    "\n",
    "1. Iterate over prompts.\n",
    "2. For each `(condition, prompt)` pair, generate multiple answers and embed them.\n",
    "3. For each **condition pair**, compute the sliced KS metrics and corresponding symmetry scores.\n",
    "4. Collect results in a DataFrame for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3045712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_condition_lookup(conditions: List[Dict]) -> Dict[str, Dict]:\n",
    "    return {c[\"name\"]: c for c in conditions}\n",
    "\n",
    "\n",
    "condition_lookup = build_condition_lookup(CONDITIONS)\n",
    "\n",
    "results = []\n",
    "\n",
    "for prompt in tqdm(PROMPTS, desc=\"Prompts\"):\n",
    "    prompt_id = prompt[\"id\"]\n",
    "    prompt_type = prompt.get(\"type\", \"unknown\")\n",
    "    en_text = prompt[\"en\"]\n",
    "    fr_text = prompt[\"fr\"]\n",
    "\n",
    "    # Cache answers and embeddings per condition\n",
    "    answers_by_condition: Dict[str, List[str]] = {}\n",
    "    emb_by_condition: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    # Generate answers and embeddings for each condition\n",
    "    for cond in CONDITIONS:\n",
    "        cond_name = cond[\"name\"]\n",
    "        answers = generate_answers_for_condition(\n",
    "            prompt_en=en_text,\n",
    "            prompt_fr=fr_text,\n",
    "            condition=cond,\n",
    "            n_samples=N_SAMPLES_PER_CONDITION,\n",
    "        )\n",
    "        answers_by_condition[cond_name] = answers\n",
    "        emb_by_condition[cond_name] = embed_texts(answers)\n",
    "\n",
    "    # Now compute symmetry metrics for each condition pair\n",
    "    for cond_a, cond_b in CONDITION_PAIRS:\n",
    "        emb_A = emb_by_condition[cond_a]\n",
    "        emb_B = emb_by_condition[cond_b]\n",
    "\n",
    "        sks_metrics = sliced_ks_distance(\n",
    "            emb_A,\n",
    "            emb_B,\n",
    "            n_directions=N_DIRECTIONS,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "        sym = symmetry_from_sks(sks_metrics)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"prompt_id\": prompt_id,\n",
    "                \"prompt_type\": prompt_type,\n",
    "                \"condition_A\": cond_a,\n",
    "                \"condition_B\": cond_b,\n",
    "                \"ks_mean\": sks_metrics.mean,\n",
    "                \"ks_std\": sks_metrics.std,\n",
    "                \"ks_sem\": sks_metrics.sem,\n",
    "                \"ks_ci_low\": sks_metrics.ci_low,\n",
    "                \"ks_ci_high\": sks_metrics.ci_high,\n",
    "                \"sym_mean\": sym[\"sym_mean\"],\n",
    "                \"sym_std\": sym[\"sym_std\"],\n",
    "                \"sym_sem\": sym[\"sym_sem\"],\n",
    "                \"sym_ci_low\": sym[\"sym_ci_low\"],\n",
    "                \"sym_ci_high\": sym[\"sym_ci_high\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83439a82",
   "metadata": {},
   "source": [
    "## Aggregate symmetry scores\n",
    "\n",
    "We can now aggregate symmetry metrics across prompts, or restrict to factual vs open-ended prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aggregate by condition pair and prompt type\n",
    "agg = (\n",
    "    results_df\n",
    "    .groupby([\"condition_A\", \"condition_B\", \"prompt_type\"])\n",
    "    .agg(\n",
    "        sym_mean=(\"sym_mean\", \"mean\"),\n",
    "        sym_std=(\"sym_mean\", \"std\"),\n",
    "        n_prompts=(\"prompt_id\", \"nunique\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e35bb1",
   "metadata": {},
   "source": [
    "You can extend this section with:\n",
    "\n",
    "- Boxplots or violin plots of `sym_mean` per prompt,\n",
    "- Separate reporting for factual vs open-ended prompts,\n",
    "- Per-prompt inspection of the most asymmetric cases (lowest `sym_mean`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilingual-llm-symmetry",
   "language": "python",
   "name": "multilingual-llm-symmetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
