{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d2f5cd",
   "metadata": {},
   "source": [
    "# Language Response Asymmetry (Cohere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f8dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36051260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "COHERE_API_KEY = os.environ.get(\"COHERE_API_KEY\")\n",
    "if not COHERE_API_KEY:\n",
    "    raise RuntimeError(\"Missing COHERE_API_KEY in environment/.env\")\n",
    "\n",
    "co = cohere.ClientV2(api_key=COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe838be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation models (examples; adjust to what your grant enables)\n",
    "MODEL_AYA = \"c4ai-aya-expanse-8b\"  # main model\n",
    "MODEL_COMMAND = \"command-r\"   # optional baseline\n",
    "\n",
    "# Embedding model\n",
    "MODEL_EMBED = \"embed-multilingual-v3.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7230d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONDITIONS = [\n",
    "    {\"name\": \"aya_en\", \"model\": MODEL_AYA, \"lang\": \"en\"},\n",
    "    {\"name\": \"aya_fr\", \"model\": MODEL_AYA, \"lang\": \"fr\"},\n",
    "    # later:\n",
    "    # {\"name\": \"aya_it\", \"model\": MODEL_AYA, \"lang\": \"it\"},\n",
    "    # {\"name\": \"aya_sw\", \"model\": MODEL_AYA, \"lang\": \"sw\"},\n",
    "]\n",
    "\n",
    "CONDITION_PAIRS = [\n",
    "    (\"aya_en\", \"aya_fr\"),\n",
    "    # later:\n",
    "    # (\"aya_en\", \"aya_it\"),\n",
    "    # (\"aya_en\", \"aya_sw\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5483abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of stochastic samples per (prompt, condition)\n",
    "N_SAMPLES_PER_CONDITION = int(os.environ.get(\"N_SAMPLES_PER_CONDITION\", 2))\n",
    "\n",
    "# Number of random projection directions for sliced KS\n",
    "N_DIRECTIONS = int(os.environ.get(\"N_DIRECTIONS\", 64))\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = int(os.environ.get(\"RANDOM_STATE\", 12345))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d0c0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def cohere_generate_answers(\n",
    "    prompt_en: str,\n",
    "    prompt_fr: str,\n",
    "    condition: Dict,\n",
    "    n_samples: int = 8,\n",
    "    max_tokens: int = 128,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    ") -> List[str]:\n",
    "    lang = condition[\"lang\"]\n",
    "    model = condition[\"model\"]\n",
    "    prompt = prompt_en if lang == \"en\" else prompt_fr\n",
    "\n",
    "    outputs = []\n",
    "    for _ in range(n_samples):\n",
    "        resp = co.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            p=top_p,                 # Cohere uses `p` for top-p in many examples/docs\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        # v2 responses can vary by SDK version; this is the common pattern:\n",
    "        text = resp.message.content[0].text if hasattr(resp, \"message\") else resp.text\n",
    "        outputs.append(text)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6104971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cohere_embed_texts(\n",
    "    texts: List[str],\n",
    "    model: str = MODEL_EMBED,\n",
    "    input_type: str = \"search_document\",\n",
    ") -> np.ndarray:\n",
    "    inputs = [{\"content\": [{\"type\": \"text\", \"text\": t}]} for t in texts]\n",
    "\n",
    "    resp = co.embed(\n",
    "        model=model,\n",
    "        inputs=inputs,\n",
    "        input_type=input_type,\n",
    "        embedding_types=[\"float\"],\n",
    "    )\n",
    "\n",
    "    # Common: resp.embeddings.float is a list[list[float]]\n",
    "    emb = resp.embeddings.float\n",
    "    return np.asarray(emb, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1ef2d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample answers: ['Penicillin was discovered by Alexander Fleming, a Scottish biologist, and pharmacist, in 1928. Fleming noticed that mold, specifically Penicillium notatum, had grown in a culture dish of Staphylococcus bacteria, causing the bacteria to deteriorate. This observation led him to conclude that the mold produced a substance that could kill or inhibit the growth of bacteria.\\n\\nFleming\\'s discovery was a significant milestone in the field of medicine and marked the beginning of the antibiotic era. He named the substance \"penicillin\" and shared his findings with other scientists, including Howard Florey and Ernst Chain, who later played crucial roles', \"Penicillin was discovered by Alexander Fleming, a Scottish bacteriologist, in 1928. Fleming noticed that a mold, later identified as Penicillium notatum, had grown in a petri dish containing Staphylococcus bacteria, causing the bacteria to deteriorate and die. This observation led to the understanding that the mold produced a substance with antibacterial properties, which he named penicillin.\\n\\nFleming's discovery was a significant milestone in the field of medicine as it marked the beginning of antibiotic therapy. His work paved the way for the development of many life-saving antibiotics that have been used to treat bacterial infections worldwide.\"]\n",
      "Embeddings shape: (2, 1024)\n"
     ]
    }
   ],
   "source": [
    "test_prompt_en = \"Who discovered penicillin?\"\n",
    "test_prompt_fr = \"Qui a découvert la pénicilline ?\"\n",
    "\n",
    "answers = cohere_generate_answers(test_prompt_en, test_prompt_fr, CONDITIONS[0], n_samples=2)\n",
    "print(\"Sample answers:\", answers)\n",
    "\n",
    "E = cohere_embed_texts(answers)\n",
    "print(\"Embeddings shape:\", E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9512d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stats_helpers import sliced_ks_distance, symmetry_from_sks\n",
    "\n",
    "generate_answers_for_condition = cohere_generate_answers\n",
    "embed_texts = cohere_embed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2acdf0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/disipio/.local/share/virtualenvs/multilingual-llm-symmetry-UDP034c6/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Prompts:  83%|████████▎ | 5/6 [00:39<00:07,  7.83s/it]\n"
     ]
    },
    {
     "ename": "TooManyRequestsError",
     "evalue": "headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-encoding': 'gzip', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'pragma': 'no-cache', 'vary': 'Origin,Accept-Encoding', 'x-accel-expires': '0', 'x-debug-trace-id': '43ac95f5f2b3db48e12c881c6062cef9', 'date': 'Mon, 22 Dec 2025 21:28:16 GMT', 'x-envoy-upstream-service-time': '5', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000', 'transfer-encoding': 'chunked'}, status_code: 429, body: {'id': '093b6f7b-bafe-45ef-a6f9-fe58251ff4ef', 'message': \"You are using a Trial key, which is limited to 20 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTooManyRequestsError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cond \u001b[38;5;129;01min\u001b[39;00m CONDITIONS:\n\u001b[32m     28\u001b[39m     cond_name = cond[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     answers = \u001b[43mgenerate_answers_for_condition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_en\u001b[49m\u001b[43m=\u001b[49m\u001b[43men_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_fr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfr_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_SAMPLES_PER_CONDITION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     answers_by_condition[cond_name] = answers\n\u001b[32m     36\u001b[39m     emb_by_condition[cond_name] = embed_texts(answers)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mcohere_generate_answers\u001b[39m\u001b[34m(prompt_en, prompt_fr, condition, n_samples, max_tokens, temperature, top_p)\u001b[39m\n\u001b[32m     16\u001b[39m outputs = []\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     resp = \u001b[43mco\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# Cohere uses `p` for top-p in many examples/docs\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# v2 responses can vary by SDK version; this is the common pattern:\u001b[39;00m\n\u001b[32m     26\u001b[39m     text = resp.message.content[\u001b[32m0\u001b[39m].text \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(resp, \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m resp.text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/multilingual-llm-symmetry-UDP034c6/lib/python3.13/site-packages/cohere/client.py:103\u001b[39m, in \u001b[36mexperimental_kwarg_decorator.<locals>._wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_kwarg(deprecated_kwarg, kwargs):\n\u001b[32m     99\u001b[39m     logger.warning(\n\u001b[32m    100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_kwarg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` parameter is an experimental feature and may change in future releases.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo suppress this warning, set `log_warning_experimental_features=False` when initializing the client.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    102\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/multilingual-llm-symmetry-UDP034c6/lib/python3.13/site-packages/cohere/client.py:35\u001b[39m, in \u001b[36mvalidate_args.<locals>._wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m     34\u001b[39m     check_fn(*args, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/multilingual-llm-symmetry-UDP034c6/lib/python3.13/site-packages/cohere/v2/client.py:360\u001b[39m, in \u001b[36mV2Client.chat\u001b[39m\u001b[34m(self, model, messages, tools, strict_tools, documents, citation_options, response_format, safety_mode, max_tokens, stop_sequences, temperature, seed, frequency_penalty, presence_penalty, k, p, logprobs, tool_choice, thinking, priority, request_options)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    218\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    219\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m     request_options: typing.Optional[RequestOptions] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    241\u001b[39m ) -> V2ChatResponse:\n\u001b[32m    242\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03m    Generates a text response to a user message and streams it down, token by token. To learn how to use the Chat API with streaming follow our [Text Generation guides](https://docs.cohere.com/v2/docs/chat-api).\u001b[39;00m\n\u001b[32m    244\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    358\u001b[39m \u001b[33;03m    )\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     _response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict_tools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict_tools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43msafety_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafety_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthinking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response.data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/multilingual-llm-symmetry-UDP034c6/lib/python3.13/site-packages/cohere/v2/raw_client.py:635\u001b[39m, in \u001b[36mRawV2Client.chat\u001b[39m\u001b[34m(self, model, messages, tools, strict_tools, documents, citation_options, response_format, safety_mode, max_tokens, stop_sequences, temperature, seed, frequency_penalty, presence_penalty, k, p, logprobs, tool_choice, thinking, priority, request_options)\u001b[39m\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnprocessableEntityError(\n\u001b[32m    625\u001b[39m         headers=\u001b[38;5;28mdict\u001b[39m(_response.headers),\n\u001b[32m    626\u001b[39m         body=typing.cast(\n\u001b[32m   (...)\u001b[39m\u001b[32m    632\u001b[39m         ),\n\u001b[32m    633\u001b[39m     )\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _response.status_code == \u001b[32m429\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequestsError(\n\u001b[32m    636\u001b[39m         headers=\u001b[38;5;28mdict\u001b[39m(_response.headers),\n\u001b[32m    637\u001b[39m         body=typing.cast(\n\u001b[32m    638\u001b[39m             typing.Optional[typing.Any],\n\u001b[32m    639\u001b[39m             construct_type(\n\u001b[32m    640\u001b[39m                 type_=typing.Optional[typing.Any],  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    641\u001b[39m                 object_=_response.json(),\n\u001b[32m    642\u001b[39m             ),\n\u001b[32m    643\u001b[39m         ),\n\u001b[32m    644\u001b[39m     )\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _response.status_code == \u001b[32m498\u001b[39m:\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidTokenError(\n\u001b[32m    647\u001b[39m         headers=\u001b[38;5;28mdict\u001b[39m(_response.headers),\n\u001b[32m    648\u001b[39m         body=typing.cast(\n\u001b[32m   (...)\u001b[39m\u001b[32m    654\u001b[39m         ),\n\u001b[32m    655\u001b[39m     )\n",
      "\u001b[31mTooManyRequestsError\u001b[39m: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-encoding': 'gzip', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'pragma': 'no-cache', 'vary': 'Origin,Accept-Encoding', 'x-accel-expires': '0', 'x-debug-trace-id': '43ac95f5f2b3db48e12c881c6062cef9', 'date': 'Mon, 22 Dec 2025 21:28:16 GMT', 'x-envoy-upstream-service-time': '5', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000', 'transfer-encoding': 'chunked'}, status_code: 429, body: {'id': '093b6f7b-bafe-45ef-a6f9-fe58251ff4ef', 'message': \"You are using a Trial key, which is limited to 20 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"}"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from prompts import PROMPTS\n",
    "\n",
    "\n",
    "def build_condition_lookup(conditions: List[Dict]) -> Dict[str, Dict]:\n",
    "    return {c[\"name\"]: c for c in conditions}\n",
    "\n",
    "\n",
    "\n",
    "condition_lookup = build_condition_lookup(CONDITIONS)\n",
    "\n",
    "results = []\n",
    "\n",
    "for prompt in tqdm(PROMPTS, desc=\"Prompts\"):\n",
    "    prompt_id = prompt[\"id\"]\n",
    "    prompt_type = prompt.get(\"type\", \"unknown\")\n",
    "    en_text = prompt[\"en\"]\n",
    "    fr_text = prompt[\"fr\"]\n",
    "\n",
    "    # Cache answers and embeddings per condition\n",
    "    answers_by_condition: Dict[str, List[str]] = {}\n",
    "    emb_by_condition: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    # Generate answers and embeddings for each condition\n",
    "    for cond in CONDITIONS:\n",
    "        cond_name = cond[\"name\"]\n",
    "        answers = generate_answers_for_condition(\n",
    "            prompt_en=en_text,\n",
    "            prompt_fr=fr_text,\n",
    "            condition=cond,\n",
    "            n_samples=N_SAMPLES_PER_CONDITION,\n",
    "        )\n",
    "        answers_by_condition[cond_name] = answers\n",
    "        emb_by_condition[cond_name] = embed_texts(answers)\n",
    "\n",
    "    # Now compute symmetry metrics for each condition pair\n",
    "    for cond_a, cond_b in CONDITION_PAIRS:\n",
    "        emb_A = emb_by_condition[cond_a]\n",
    "        emb_B = emb_by_condition[cond_b]\n",
    "\n",
    "        sks_metrics = sliced_ks_distance(\n",
    "            emb_A,\n",
    "            emb_B,\n",
    "            n_directions=N_DIRECTIONS,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "        sym = symmetry_from_sks(sks_metrics)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"prompt_id\": prompt_id,\n",
    "                \"prompt_type\": prompt_type,\n",
    "                \"condition_A\": cond_a,\n",
    "                \"condition_B\": cond_b,\n",
    "                \"ks_mean\": sks_metrics.mean,\n",
    "                \"ks_std\": sks_metrics.std,\n",
    "                \"ks_sem\": sks_metrics.sem,\n",
    "                \"ks_ci_low\": sks_metrics.ci_low,\n",
    "                \"ks_ci_high\": sks_metrics.ci_high,\n",
    "                \"sym_mean\": sym[\"sym_mean\"],\n",
    "                \"sym_std\": sym[\"sym_std\"],\n",
    "                \"sym_sem\": sym[\"sym_sem\"],\n",
    "                \"sym_ci_low\": sym[\"sym_ci_low\"],\n",
    "                \"sym_ci_high\": sym[\"sym_ci_high\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ebb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilingual-llm-symmetry",
   "language": "python",
   "name": "multilingual-llm-symmetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
