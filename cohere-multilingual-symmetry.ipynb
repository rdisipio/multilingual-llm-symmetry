{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d2f5cd",
   "metadata": {},
   "source": [
    "# Language Response Asymmetry (Cohere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f8dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36051260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "COHERE_API_KEY = os.environ.get(\"COHERE_API_KEY\")\n",
    "if not COHERE_API_KEY:\n",
    "    raise RuntimeError(\"Missing COHERE_API_KEY in environment/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c61f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.ClientV2(api_key=COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe838be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation models (examples; adjust to what your grant enables)\n",
    "MODEL_AYA = \"c4ai-aya-expanse-8b\"  # main model\n",
    "MODEL_COMMAND = \"command-r\"   # optional baseline\n",
    "\n",
    "# Embedding model\n",
    "MODEL_EMBED = \"embed-multilingual-v3.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7230d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONDITIONS = [\n",
    "    {\"name\": \"aya_en\", \"model\": MODEL_AYA, \"lang\": \"en\"},\n",
    "    {\"name\": \"aya_fr\", \"model\": MODEL_AYA, \"lang\": \"fr\"},\n",
    "    # later:\n",
    "    # {\"name\": \"aya_it\", \"model\": MODEL_AYA, \"lang\": \"it\"},\n",
    "    # {\"name\": \"aya_sw\", \"model\": MODEL_AYA, \"lang\": \"sw\"},\n",
    "]\n",
    "\n",
    "CONDITION_PAIRS = [\n",
    "    (\"aya_en\", \"aya_fr\"),\n",
    "    # later:\n",
    "    # (\"aya_en\", \"aya_it\"),\n",
    "    # (\"aya_en\", \"aya_sw\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5483abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of stochastic samples per (prompt, condition)\n",
    "N_SAMPLES_PER_CONDITION = int(os.environ.get(\"N_SAMPLES_PER_CONDITION\", 2))\n",
    "\n",
    "# Number of random projection directions for sliced KS\n",
    "N_DIRECTIONS = int(os.environ.get(\"N_DIRECTIONS\", 64))\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = int(os.environ.get(\"RANDOM_STATE\", 12345))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d0c0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def cohere_generate_answers(\n",
    "    prompt_en: str,\n",
    "    prompt_fr: str,\n",
    "    condition: Dict,\n",
    "    n_samples: int = 8,\n",
    "    max_tokens: int = 64,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    ") -> List[str]:\n",
    "    lang = condition[\"lang\"]\n",
    "    model = condition[\"model\"]\n",
    "    prompt = prompt_en if lang == \"en\" else prompt_fr\n",
    "\n",
    "    outputs = []\n",
    "    for _ in range(n_samples):\n",
    "        resp = co.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            p=top_p,                 # Cohere uses `p` for top-p in many examples/docs\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        # v2 responses can vary by SDK version; this is the common pattern:\n",
    "        text = resp.message.content[0].text if hasattr(resp, \"message\") else resp.text\n",
    "        outputs.append(text)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6104971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cohere_embed_texts(\n",
    "    texts: List[str],\n",
    "    model: str = MODEL_EMBED,\n",
    "    input_type: str = \"search_document\",\n",
    ") -> np.ndarray:\n",
    "    inputs = [{\"content\": [{\"type\": \"text\", \"text\": t}]} for t in texts]\n",
    "\n",
    "    resp = co.embed(\n",
    "        model=model,\n",
    "        inputs=inputs,\n",
    "        input_type=input_type,\n",
    "        embedding_types=[\"float\"],\n",
    "    )\n",
    "\n",
    "    # Common: resp.embeddings.float is a list[list[float]]\n",
    "    emb = resp.embeddings.float\n",
    "    return np.asarray(emb, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1ef2d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample answers: ['Penicillin was discovered by Alexander Fleming, a Scottish biologist, and pharmacist, in 1928. Fleming noticed that mold, specifically Penicillium notatum, had grown in a culture dish of Staphylococcus bacteria, causing the bacteria to deteriorate. This observation led him to conclude that the mold produced a substance that could kill or inhibit the growth of bacteria.\\n\\nFleming\\'s discovery was a significant milestone in the field of medicine and marked the beginning of the antibiotic era. He named the substance \"penicillin\" and shared his findings with other scientists, including Howard Florey and Ernst Chain, who later played crucial roles', \"Penicillin was discovered by Alexander Fleming, a Scottish bacteriologist, in 1928. Fleming noticed that a mold, later identified as Penicillium notatum, had grown in a petri dish containing Staphylococcus bacteria, causing the bacteria to deteriorate and die. This observation led to the understanding that the mold produced a substance with antibacterial properties, which he named penicillin.\\n\\nFleming's discovery was a significant milestone in the field of medicine as it marked the beginning of antibiotic therapy. His work paved the way for the development of many life-saving antibiotics that have been used to treat bacterial infections worldwide.\"]\n",
      "Embeddings shape: (2, 1024)\n"
     ]
    }
   ],
   "source": [
    "test_prompt_en = \"Who discovered penicillin?\"\n",
    "test_prompt_fr = \"Qui a découvert la pénicilline ?\"\n",
    "\n",
    "answers = cohere_generate_answers(test_prompt_en, test_prompt_fr, CONDITIONS[0], n_samples=2)\n",
    "print(\"Sample answers:\", answers)\n",
    "\n",
    "E = cohere_embed_texts(answers)\n",
    "print(\"Embeddings shape:\", E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9512d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stats_helpers import sliced_ks_distance, symmetry_from_sks\n",
    "\n",
    "generate_answers_for_condition = cohere_generate_answers\n",
    "embed_texts = cohere_embed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2acdf0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/disipio/.local/share/virtualenvs/multilingual-llm-symmetry-UDP034c6/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Prompts: 100%|██████████| 6/6 [00:26<00:00,  4.38s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_type</th>\n",
       "      <th>condition_A</th>\n",
       "      <th>condition_B</th>\n",
       "      <th>ks_mean</th>\n",
       "      <th>ks_std</th>\n",
       "      <th>ks_sem</th>\n",
       "      <th>ks_ci_low</th>\n",
       "      <th>ks_ci_high</th>\n",
       "      <th>sym_mean</th>\n",
       "      <th>sym_std</th>\n",
       "      <th>sym_sem</th>\n",
       "      <th>sym_ci_low</th>\n",
       "      <th>sym_ci_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fact_1</td>\n",
       "      <td>factual</td>\n",
       "      <td>aya_en</td>\n",
       "      <td>aya_fr</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.233588</td>\n",
       "      <td>0.029199</td>\n",
       "      <td>0.786521</td>\n",
       "      <td>0.900979</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.233588</td>\n",
       "      <td>0.029199</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.213479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fact_2</td>\n",
       "      <td>factual</td>\n",
       "      <td>aya_en</td>\n",
       "      <td>aya_fr</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.247507</td>\n",
       "      <td>0.030938</td>\n",
       "      <td>0.736236</td>\n",
       "      <td>0.857514</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>0.247507</td>\n",
       "      <td>0.030938</td>\n",
       "      <td>0.142486</td>\n",
       "      <td>0.263764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fact_3</td>\n",
       "      <td>factual</td>\n",
       "      <td>aya_en</td>\n",
       "      <td>aya_fr</td>\n",
       "      <td>0.914062</td>\n",
       "      <td>0.190127</td>\n",
       "      <td>0.023766</td>\n",
       "      <td>0.867481</td>\n",
       "      <td>0.960644</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.190127</td>\n",
       "      <td>0.023766</td>\n",
       "      <td>0.039356</td>\n",
       "      <td>0.132519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>open_1</td>\n",
       "      <td>open</td>\n",
       "      <td>aya_en</td>\n",
       "      <td>aya_fr</td>\n",
       "      <td>0.820312</td>\n",
       "      <td>0.241805</td>\n",
       "      <td>0.030226</td>\n",
       "      <td>0.761070</td>\n",
       "      <td>0.879555</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>0.241805</td>\n",
       "      <td>0.030226</td>\n",
       "      <td>0.120445</td>\n",
       "      <td>0.238930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>open_2</td>\n",
       "      <td>open</td>\n",
       "      <td>aya_en</td>\n",
       "      <td>aya_fr</td>\n",
       "      <td>0.773438</td>\n",
       "      <td>0.250867</td>\n",
       "      <td>0.031358</td>\n",
       "      <td>0.711975</td>\n",
       "      <td>0.834900</td>\n",
       "      <td>0.226562</td>\n",
       "      <td>0.250867</td>\n",
       "      <td>0.031358</td>\n",
       "      <td>0.165100</td>\n",
       "      <td>0.288025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>open_3</td>\n",
       "      <td>open</td>\n",
       "      <td>aya_en</td>\n",
       "      <td>aya_fr</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.196699</td>\n",
       "      <td>0.024587</td>\n",
       "      <td>0.858059</td>\n",
       "      <td>0.954441</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.196699</td>\n",
       "      <td>0.024587</td>\n",
       "      <td>0.045559</td>\n",
       "      <td>0.141941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_id prompt_type condition_A condition_B   ks_mean    ks_std    ks_sem  \\\n",
       "0    fact_1     factual      aya_en      aya_fr  0.843750  0.233588  0.029199   \n",
       "1    fact_2     factual      aya_en      aya_fr  0.796875  0.247507  0.030938   \n",
       "2    fact_3     factual      aya_en      aya_fr  0.914062  0.190127  0.023766   \n",
       "3    open_1        open      aya_en      aya_fr  0.820312  0.241805  0.030226   \n",
       "4    open_2        open      aya_en      aya_fr  0.773438  0.250867  0.031358   \n",
       "5    open_3        open      aya_en      aya_fr  0.906250  0.196699  0.024587   \n",
       "\n",
       "   ks_ci_low  ks_ci_high  sym_mean   sym_std   sym_sem  sym_ci_low  \\\n",
       "0   0.786521    0.900979  0.156250  0.233588  0.029199    0.099021   \n",
       "1   0.736236    0.857514  0.203125  0.247507  0.030938    0.142486   \n",
       "2   0.867481    0.960644  0.085938  0.190127  0.023766    0.039356   \n",
       "3   0.761070    0.879555  0.179688  0.241805  0.030226    0.120445   \n",
       "4   0.711975    0.834900  0.226562  0.250867  0.031358    0.165100   \n",
       "5   0.858059    0.954441  0.093750  0.196699  0.024587    0.045559   \n",
       "\n",
       "   sym_ci_high  \n",
       "0     0.213479  \n",
       "1     0.263764  \n",
       "2     0.132519  \n",
       "3     0.238930  \n",
       "4     0.288025  \n",
       "5     0.141941  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from prompts import PROMPTS\n",
    "\n",
    "\n",
    "def build_condition_lookup(conditions: List[Dict]) -> Dict[str, Dict]:\n",
    "    return {c[\"name\"]: c for c in conditions}\n",
    "\n",
    "\n",
    "\n",
    "condition_lookup = build_condition_lookup(CONDITIONS)\n",
    "\n",
    "results = []\n",
    "\n",
    "for prompt in tqdm(PROMPTS, desc=\"Prompts\"):\n",
    "    prompt_id = prompt[\"id\"]\n",
    "    prompt_type = prompt.get(\"type\", \"unknown\")\n",
    "    en_text = prompt[\"en\"]\n",
    "    fr_text = prompt[\"fr\"]\n",
    "\n",
    "    # Cache answers and embeddings per condition\n",
    "    answers_by_condition: Dict[str, List[str]] = {}\n",
    "    emb_by_condition: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    # Generate answers and embeddings for each condition\n",
    "    for cond in CONDITIONS:\n",
    "        cond_name = cond[\"name\"]\n",
    "        answers = generate_answers_for_condition(\n",
    "            prompt_en=en_text,\n",
    "            prompt_fr=fr_text,\n",
    "            condition=cond,\n",
    "            n_samples=N_SAMPLES_PER_CONDITION,\n",
    "        )\n",
    "        answers_by_condition[cond_name] = answers\n",
    "        emb_by_condition[cond_name] = embed_texts(answers)\n",
    "\n",
    "    # Now compute symmetry metrics for each condition pair\n",
    "    for cond_a, cond_b in CONDITION_PAIRS:\n",
    "        emb_A = emb_by_condition[cond_a]\n",
    "        emb_B = emb_by_condition[cond_b]\n",
    "\n",
    "        sks_metrics = sliced_ks_distance(\n",
    "            emb_A,\n",
    "            emb_B,\n",
    "            n_directions=N_DIRECTIONS,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "        sym = symmetry_from_sks(sks_metrics)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"prompt_id\": prompt_id,\n",
    "                \"prompt_type\": prompt_type,\n",
    "                \"condition_A\": cond_a,\n",
    "                \"condition_B\": cond_b,\n",
    "                \"ks_mean\": sks_metrics.mean,\n",
    "                \"ks_std\": sks_metrics.std,\n",
    "                \"ks_sem\": sks_metrics.sem,\n",
    "                \"ks_ci_low\": sks_metrics.ci_low,\n",
    "                \"ks_ci_high\": sks_metrics.ci_high,\n",
    "                \"sym_mean\": sym[\"sym_mean\"],\n",
    "                \"sym_std\": sym[\"sym_std\"],\n",
    "                \"sym_sem\": sym[\"sym_sem\"],\n",
    "                \"sym_ci_low\": sym[\"sym_ci_low\"],\n",
    "                \"sym_ci_high\": sym[\"sym_ci_high\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb9ebb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition_A</th>\n",
       "      <th>condition_B</th>\n",
       "      <th>prompt_type</th>\n",
       "      <th>sym_mean</th>\n",
       "      <th>sym_std</th>\n",
       "      <th>n_prompts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aya_en</td>\n",
       "      <td>aya_fr</td>\n",
       "      <td>factual</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>0.058983</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aya_en</td>\n",
       "      <td>aya_fr</td>\n",
       "      <td>open</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.067357</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  condition_A condition_B prompt_type  sym_mean   sym_std  n_prompts\n",
       "0      aya_en      aya_fr     factual  0.148438  0.058983          3\n",
       "1      aya_en      aya_fr        open  0.166667  0.067357          3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate by condition pair and prompt type\n",
    "agg = (\n",
    "    results_df\n",
    "    .groupby([\"condition_A\", \"condition_B\", \"prompt_type\"])\n",
    "    .agg(\n",
    "        sym_mean=(\"sym_mean\", \"mean\"),\n",
    "        sym_std=(\"sym_mean\", \"std\"),\n",
    "        n_prompts=(\"prompt_id\", \"nunique\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed1841",
   "metadata": {},
   "source": [
    "### Note on Inuktitut (IU) as an Out-of-Distribution Language\n",
    "\n",
    "Inuktitut is not part of the declared language coverage of the models evaluated here. Nevertheless, the models can still be prompted with Inuktitut text, and will produce outputs conditioned on that input, even if the tokenizer, internal representations, or generation behavior are not optimized for this language. In this setting, we do not assume fluent or correct Inuktitut generation. Instead, we treat Inuktitut as an **out-of-distribution stress test** for multilingual robustness. Model outputs generated from Inuktitut prompts are translated back to English using a fixed translation system, and the resulting English texts are used for embedding and distributional comparison. This translation step is treated as a measurement instrument rather than ground truth, allowing us to assess how much semantic signal survives when the model is driven through an extreme low-resource linguistic channel.\n",
    "Since the IU language is not included, the model will probably try to answer in English. We can try to do some prompt engineering, for example\n",
    "\n",
    "\"Please answer in Inuktitut. Do not switch languages. If unsure, still respond in Inuktitut.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be341092",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilingual-llm-symmetry",
   "language": "python",
   "name": "multilingual-llm-symmetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
